<!doctype html><html><head><title>Distributed consensus - Raft and JRaft · dromara(Open source organization)</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="The girl on the prairie"><meta name=generator content="Hugo 0.55.5"><link rel="shortcut icon" href=/img/favicon.png type=image/png><link href=https://unpkg.com/purecss@1.0.0/build/base-min.css rel=stylesheet><link href=/css/main.css rel=stylesheet><link href=/css/zoom-image.css rel=stylesheet><script src=/js/iconfont.js></script><script src=/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad();</script><script>window.SITE_LANGUAGE="en"</script><script src=/js/app.js></script><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-187960192-2','auto');ga('send','pageview');}</script></head><body><header class=ss-header><nav class=navbar role=navigation aria-label="main navigation"><div class=navbar-brand><a class=logo-link href=/en/><img class=logo src=/img/logo.png></a><div class=-show-mobile><a id=mobile-menu-icon><svg class="icon" aria-hidden="true"><use xlink:href="#icon-menu"/></svg></a><nav id=mobile-menu><div id=js-menu-search-mobile class=navbar-search-mobile><input class=input placeholder=Search><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search"/></svg></div><a href=/en/projects/><span>Projects</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"/></svg></a>
<a href=/en/guides/><span>Guides</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"/></svg></a>
<a href=/en/blog/><span>Blog</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"/></svg></a>
<a href=/en/activities/><span>Activity</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"/></svg></a>
<a href=/awesome/><span>Awesome Dromara</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-ARROW"/></svg></a>
<a href=/projects/sofa-jraft/consistency-raft-jraft/><span>中文</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-transfer"/></svg></a></nav></div></div><div class="navbar-menu -hidden-mobile"><div class=navbar-start><a class=navbar-item href=/en/projects/>Projects</a>
<a class=navbar-item href=/en/guides/>Guides</a>
<a class=navbar-item href=/en/blog/>Blog</a>
<a class=navbar-item href=/en/activities/>Activity</a>
<a class=navbar-item href=/awesome/>Awesome Dromara</a></div><div class=navbar-end><div class=navbar-item><div id=js-menu-search class=navbar-search><input class=input placeholder=Search><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search"/></svg></div></div><div class=navbar-item><a class=translation href=/projects/sofa-jraft/consistency-raft-jraft/>中</a></div></div></div></nav></header><div class=ss-layout-container><aside class="ss-layout-aside -left ss-card -soft-hidden"></aside><main class="ss-layout-main -card"><div class=ss-meta><div class=container><h1 class=title>Distributed consensus - Raft and JRaft</h1><a class="edit-button -hidden-mobile" href=https://github.com/dromara/edit/master/content/en/projects/sofa-jraft/consistency-raft-jraft/index.md>Edit</a></div><div class=meta>Update time: 2021-01-24</div></div><article class=typo><h1 id=distributed-consensus-algorithm>Distributed consensus algorithm</h1><h2 id=understand-distributed-consensus>Understand distributed consensus</h2><ul><li><strong>Multiple participants</strong> reach a <strong>complete consensus</strong> on <strong>one thing</strong>: one conclusion for one thing.</li><li>The conclusion cannot be overthrown.</li></ul><h2 id=typical-distributed-consensus-algorithms>Typical distributed consensus algorithms</h2><ul><li>Paxos: It is considered as the foundation of distributed consensus algorithms. Other algorithms are its variants. However, the Paxos paper only provides the process of a single proposal, without describing the details of multi-paxos that is required for state machine replication. Paxos implementation involves high engineering complexity, for example, multiple-point writes and log hole tolerance.</li><li>Zab: It is applied in ZooKeeper and widely used in the industry. However, it is not available as a universal library.</li><li>Raft: It is known for being easy to understand. There are many renowned Raft implementations in the industry, such as etcd, Braft, and TiKV.</li></ul><h1 id=introduction-to-raft>Introduction to Raft</h1><p><a href=https://raft.github.io/>Raft</a> is in nature a Paxos-based distributed consensus algorithm that is much easier to understand than Paxos. Unlike Paxos, Raft divides the protocols into independent modules, and uses a streamlined design, making the Raft protocol easier to implement.</p><p>Specifically, Raft divides consensus protocols into almost completely decoupled modules, such as leader election, membership change, log replication, and snapshot.</p><p>Raft adopts a more streamlined design by preventing reordering commits, simplifying roles (it has only three roles: leader, follower, and candidate), allowing only the leader to write, and using randomized timeout values to design leader election.</p><h2 id=feature-strong-leader>Feature: strong leader</h2><ol><li>The system can have only one leader at the same time, and only the leader can accept requests sent by clients.</li><li>The leader is responsible for communication with all followers, sending proposals to all followers, and receiving responses from the majority of followers.</li><li>The leader also needs to send heartbeats to all followers to maintain its leadership.</li></ol><p>To summarize, a strong leader tells its followers: <strong>&ldquo;Do not say anything. Do what I said and let me know when you finish!&rdquo;</strong>
In addition, a leader must always remain active by sending heartbeats to followers. Otherwise, a follower will take its place.</p><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*5Cw5Qp1oyYQAAAAAAAAAAABjARQnAQ alt="strong-leader.png | left | 350x250"></p><h2 id=replicated-state-machine>Replicated state machine</h2><p>Assume we have an infinitely incrementing sequence (system) a[1, 2, 3…]. If for any integer i, the value of a[i] meets the distributed consensus requirement, the system meets the requirement of a consensus state machine.
Basically, all real life systems are subject to continuous operations, and reaching consensus on a single value is definitely not enough. To make sure all replicas of a real life system are consistent, we usually convert the operations into entries of a <a href=https://en.wikipedia.org/wiki/Write-ahead_logging>write-ahead-log</a>(WAL). Then, we make sure all replicas of the system reach a consensus on the WAL entries, so that each replica performs operations of the WAL entries in order. As a result, the replicas are in consistent states.</p><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*OiwGTZnO2uMAAAAAAAAAAABjARQnAQ alt="st.png | left | 450x250"></p><ol><li>A client sent a write (operation) request to the leader.</li><li>The leader converts the &ldquo;operation&rdquo; request into a WAL entry, and appends the entry to local log. When doing so, the leader replicates the log entry to all followers.</li><li>After receiving responses from the majority of followers, the leader applies the corresponding &ldquo;operation&rdquo; of the log entry to the state machine.</li><li>The leader then returns the result to client.</li></ol><h2 id=basic-concepts-of-raft>Basic concepts of Raft</h2><h3 id=three-roles-states-of-a-raft-node>Three roles/states of a Raft node</h3><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*EK6gQYwiBXkAAAAAAAAAAABjARQnAQ alt="raft-node | left | 400x250"></p><ol><li>A follower is completely passive and cannot issue any requests. It simply responds to remote procedure calls (RPCs) from the leader and the candidate. When servers start up, they begin as followers.</li><li>A leader handles all client requests and replicates log entries to all followers.</li><li>A candidate can be elected as a new leader. A follower becomes a candidate when the election times out.</li></ol><h3 id=three-types-of-rpcs>Three types of RPCs</h3><ol><li>RequestVote RPC: A candidate issues RequestVote RPCs to other nodes to request for votes.</li><li>AppendEntries (heartbeat) RPC: A leader issues AppendEntries RPCs to replicate log entries to followers or send heartbeats (AppendEntries RPCs that carry no log entries).</li><li>InstallSnapshot RPC: A leader issues InstallSnapshot RPCs to send chunks of a snapshot to followers. Although in most cases each server creates snapshots independently, the leader must send snapshots to some followers that are too far behind. This usually happens when the leader has discarded the next log entry to be sent to a follower (removed during log compaction).</li></ol><h3 id=terms-logical-clock>Terms (logical clock)</h3><ol><li>Raft divides time into terms of arbitrary length. Terms are numbered with monotonically incrementing integers (term IDs).</li><li>Each term starts with a new leader election. If a candidate wins an election, it serves as the leader and manages the entire cluster within the term. Therefore, a term comprises <strong>leader election + normal operations.</strong></li><li>There is at most one leader in a given term. In some situations, an election will result in a split vote, which will end with no leader.</li></ol><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*CTpYRa_CB_4AAAAAAAAAAABjARQnAQ alt="term.png | left | 500x200"></p><h2 id=raft-function-breakdown>Raft function breakdown</h2><h3 id=leader-election>Leader election</h3><ul><li>Timeout-driven: Heartbeat/election timeout</li><li>Randomized election timeout: This reduces the chances of split votes in the case of election conflicts.</li><li>Election process:<ul><li>Follower &gt; Candidate (triggered by election timeout)<ul><li>The candidate wins the election: Candidate &gt; Leader</li><li>Another node wins the election: Candidate &gt; Follower</li><li>Election ends with no winner: Candidate &gt; Candidate</li></ul></li></ul></li><li>Election actions:<ul><li>A follower increments its current term and transitions to candidate state.</li><li>The candidate votes for itself and issues RequestVote RPCs in parallel to each of other nodes.</li></ul></li><li>New leader election principles (maximum commit principle)<ul><li>Candidates include log information in RequestVote RPCs(index &amp; term of last log entry).</li><li>During elections, choose a candidate with logs which are most likely to contain all committed entries.</li><li>Voting server V denies a vote if its log is “more complete”:
(lastTermV &gt; lastTermC) ||
((lastTermV == lastTermC) &amp;&amp; (lastIndexV &gt; lastIndexC))</li><li>The leader will have the “most complete” log among the electing majority.</li></ul></li><li>Election safety: At most one leader can be elected in a given term. A term can end up with no leaders. Candidates can start a new election in the next term.</li></ul><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*vC1PR4snguoAAAAAAAAAAABjARQnAQ alt="safe-term | left | 450x80"></p><ul><li>Time parameters that affect the rate of success of Raft leader election:<ul><li>Round trip time (RTT): network latency.</li><li>Heartbeat timeout: heartbeat interval. Generally, the heartbeat interval should be in a smaller magnitude than the election timeout period, so that the leader can constantly send heartbeats to prevent followers from transitioning to candidates and initiating elections.</li><li>Election timeout: The period of time that followers would wait for communication from the leader before they start an election.</li><li>Meantime between failures (MTBF): The mean time between failures of a system during normal system operation.
<code>RTT &lt;&lt; Heartbeat timeout &lt; Election timeout (ET) &lt;&lt; MTBF</code></li></ul></li><li>Randomized election timeouts: <code>Random (ET, 2 ET)</code></li></ul><h3 id=log-replication>Log replication</h3><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*dY0aTYArhPIAAAAAAAAAAABjARQnAQ alt="log-replication | left | 450x200"></p><ul><li>Raft log format<ul><li><code>(TermId, LogIndex, and LogValue)</code></li><li><code>TermId and LogIndex</code> determine a unique log entry.</li></ul></li><li>Log replication key points<ul><li>Continuity: Log holes are not allowed.</li><li>Validity:<ul><li>If two logs of different nodes contain an entry with the same logindex and term, the logs are identical in log values.</li><li>Log entries on the leader are always valid.</li><li>The validity of a follower&rsquo;s log is determined based on the result of comparison with the log on the leader.</li></ul></li></ul></li><li>Validity check of followers&rsquo; logs.<ul><li>An AppendEntries RPC carries a unique identifier of the previous log entry <code>(prevTermId, prevLogIndex).</code></li><li>Recursive derivation</li></ul></li><li>Recovery of followers&rsquo; logs<ul><li>The leader progressively decreases the nextIndex and re-issues AppendEntries RPCs until followers&rsquo; logs are consistent with the leader&rsquo;s log.</li></ul></li></ul><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*8xqPR7ZR7EsAAAAAAAAAAABjARQnAQ alt="log-replication-2.png | left | 400x150"></p><h3 id=working-mechanism-of-commitindex>Working mechanism of commitIndex</h3><ul><li>CommitIndex <code>(TermId, LogIndex)</code>:<ul><li>Simply put, commitIndex is the position of a log entry in the log which the leader has replicated to a majority of servers and decides to apply to the state machine.</li><li>When a log entry is replicated to followers, it is persisted, but cannot be immediately applied to the state machine.</li><li>Only the leader knows whether this log entry has been stored on a majority of servers and is ready to be applied to the state machine.</li><li>Followers record the current commitIndex sent by the leader, and all log entries whose indexes are smaller than or equal to the commitIndex can be applied to the state machine.</li></ul></li><li>How commitIndex works?<ul><li>The leader carries the current commitIndex in the next AppendEntries RPC (including heartbeats).</li><li>Followers check the validity of the log entry. If the log entry is valid, they accept the AppendEntries RPC and simultaneously update their local commitIndexes. Then they apply log entries whose indexes are smaller than or equal to the commitIndex to the state machine.</li></ul></li></ul><h3 id=appendentries-rpc>AppendEntries RPC</h3><ul><li>A complete RPC contains the following information: currentTerm, logEntries[], prevTerm, prevLogIndex, commitTerm, and commitLogIndex.</li><li>currentTerm and logEntries[]: log information. For the sake of efficiency, multiple log-entries are processed at a time.</li><li>prevTerm and prevLogIndex: used for log validity check.</li><li>commitTerm and commitLogIndex: position of the latest committed log entry (commitIndex).</li></ul><h3 id=summary-what-can-we-do-with-raft-now>Summary: What can we do with Raft now?</h3><ul><li>Continuously determine multiple proposals to ensure that the states of all system nodes in the cluster are completely consistent.</li><li>Automatically elect the leader, and ensure the availability of the system when only a minority of nodes are down.</li><li>Logs of all servers are highly synchronized, which ensures zero data loss after any server in the cluster is down.</li></ul><h1 id=introduction-to-jraft>Introduction to JRaft</h1><p>JRaft a production-level high-performance Java implementation based on the <a href=https://raft.github.io/>RAFT</a> the consensus algorithm. It supports MULTI-RAFT-GROUP and is suitable for high-load and low-latency scenarios. JRaft allows you to focus on your own business area, and leave all RAFT-related technical problems to JRaft. JRaft is easy to use, and you can master it in a very short period by reading a few examples.</p><p>JRaft was transplanted from Baidu&rsquo;s <a href=https://github.com/brpc/braft>Braft</a>, upon which we have made some optimization and improvement. Many thanks to the Baidu Braft team for opening source such an excellent C++ RAFT implementation.</p><h2 id=overall-features-and-performance-optimization-of-jraft>Overall features and performance optimization of JRaft</h2><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*68HaTJZQxVUAAAAAAAAAAABjARQnAQ alt="feature | left | 500x450"></p><h3 id=supported-features>Supported features</h3><ul><li>Leader election: We have already introduced the leader election mechanism of Raft earlier in this topic.</li><li>Log replication and recovery:<ul><li>Log replication ensures that committed data is not lost and is successfully replicated to a majority of servers.</li><li>Log recovery involves two types:<ul><li>Current term log recovery<ul><li>It mainly deals with log recovery of some follower nodes that join the cluster after a restart or new follower nodes that have recently joined the cluster.</li></ul></li><li>Prev term log recovery<ul><li>It mainly deals with the log consistency problem caused by leader change.</li></ul></li></ul></li></ul></li><li>Snapshot and log compaction: JRaft regularly generates snapshots to implement log compaction, which speeds up the startup of servers and their log recovery. It issues InstallSnapshot RPCs to send chunks of a snapshot to followers, as shown in the following figure.</li></ul><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*a3xDT5mfSP4AAAAAAAAAAABjARQnAQ alt="snapshot.png | left | 250x200"></p><ul><li><p>Membership change: used for online cluster configuration changes, such adding, deleting, and replacing nodes.</p></li><li><p>Transfer leader: a proactive leader change when you restart a node for maintenance or leader load balancing.</p></li><li><p>Partition tolerance of Symmetric network:</p></li></ul><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*N_rBQ6oKsv4AAAAAAAAAAABjARQnAQ alt="symmetric-net-partition-tolerance | left | 200x150"></p><p>As shown in the above figure, S1 is the current leader. Due to a communication failure caused by network partitioning, follower S2 did not receive heartbeats from the current leader. It transitions to a candidate and constantly increments its term in attempts to start a new election in the cluster. Once the communication is recovered with network recovery, S2 will start a new election in the cluster, force the current leader to step down, and cause service interruption. To avoid this problem, JRaft introduces a pre-vote mechanism.
- In JRaft, a follower must issue pre-vote RPCs (currentTerm + 1, lastLogIndex, lastLogTerm) and obtain votes from a majority of nodes before it can turn into a candidate and issue real RequestVote RPCs. Due to a network failure, a partitioned follower node cannot issue pre-vote RPCs to other nodes, and cannot win the pre-vote. Therefore, it will not be able to transition to a candidate to start a new election. This reduces the chance of service interruptions caused by unnecessary leader election.</p><ul><li>Asymmetric network partition tolerance:</li></ul><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*sHgoQa2jywwAAAAAAAAAAABjARQnAQ alt="asymmetric-net-partition-tolerance | left | 200x150"></p><p>As shown in the above figure, S1 is the current leader. The communication between S1 and S2 fails due to network partitioning, but S2 is still able to communicate with follower S3. Therefore, S2&rsquo;s pre-vote RPC is responded by S3, and wins the majority vote to transition to a candidate. S2 constantly triggers timeout elections and increments its term, while sending RequestVote RPCs to S3. As a result, S3 increments its term to keep consistent with the term of S2, which is higher than the leader&rsquo;s term. This causes S3 to reject log replication from the leader.
- Every follower in JRaft maintains a local timestamp to record the time of AppendEntries RPCs (including heartbeats) from the leader. A follower can only accept pre-vote RPCs after the election timeout.</p><ul><li><p>Fault tolerance: Failures on a minority of servers do not impact the service availability of the overall system. Such server failures include but are not limited to:</p><ul><li>Server power outage</li><li>App force-stop</li><li>Slow responses on servers (due to garbage collection or out-of-memory errors)</li><li>Network failures</li><li>Other problems that prevent a Raft node from working normally</li></ul></li><li><p>Workaround when quorate peers are dead: When the majority of servers fail, the entire cluster is no longer available. The safe way is to wait for most servers to recover, so that data security can be guaranteed. However, if you prefer maximum system availability and accepts certain degrees of data inconsistency, you can use the reset_peers command of JRaft to quickly reset the entire cluster and resume cluster availability.</p></li><li><p>Metrics: JRaft offers a variety of built-in statistical performance metrics based on the <a href=https://metrics.dropwizard.io/4.0.0/getting-started.html>metrics</a> class library. These metrics can help you quickly and easily find out your system performance bottlenecks.</p></li><li><p>Jepsen: In addition to hundreds of unit tests and some chaos tests, JRaft also uses a distributed verification and fault injection testing framework <a href=https://github.com/jepsen-io/jepsen>Jepsen</a> to simulate many situations, and has passed all these tests:</p><ul><li>Randomized partitioning with two partitions: a big one and a small one</li><li>Randomly adding and removing nodes</li><li>Randomly stopping and starting nodes</li><li>Randomly kill -9 and starting nodes</li><li>Randomly dividing a cluster into two groups, with one node connection the two to simulate network partitioning</li><li>Randomly dividing a cluster into different majority groups</li></ul></li></ul><h3 id=performance-optimization>Performance optimization</h3><p>In addition to functional integrity, JRaft has done a lot of performance optimization. Here is some <a href=https://github.com/alipay/sofa-jraft/wiki/Benchmark-%E6%95%B0%E6%8D%AE>benchmark</a> data of a KV scene (get/put). With small data packets and a read/write ratio of 9:1, when linearizable reads are guaranteed, a three-replica cluster can achieve up to 400,000+ ops.</p><p>The following describes themajor optimizations:</p><ul><li>Batch: We know that the secrets to the success of the Internet are &ldquo;cache&rdquo; and &ldquo;batch&rdquo;. JRaft has done a lot of work on batch operations, and almost the entire workflow is implemented with batch operations. JRaft significantly improves the overall performance through batch consumption of the disruptor&rsquo;s MPSC module, including but not limited to:<ul><li>Batch task submitting</li><li>Batch network sending</li><li>Local I/O batch writes<ul><li>To ensure zero log loss, every log entry is subject to fsync flush, which is time consuming. In JRaft, log entries are written in batches.</li></ul></li><li>Batch applying log entries to state machine
Although a lot of batch operations are used in JRaft, these operations do not delay any single request for batching.</li></ul></li><li>Replication pipeline: Generally, log synchronization between the leader and the follower nodes is implemented through a serial batch operation. After a batch is sent, the leader needs to wait for the batch to be synchronized before it can send another batch (ping-pong ), causing a rather long delay. In JRaft, log entries are replicated through the pipeline between the leader and follower nodes, which significantly reduces the data synchronization delay and improves the throughput. Based on our test result, scenarios with pipeline replication enabled can have 30% higher throughput. For more information, see <a href=https://github.com/alipay/sofa-jraft/wiki/Benchmark-%E6%95%B0%E6%8D%AE>benchmark</a>.</li><li>Append log in parallel: In JRaft, the leader replicates log entries to followers in parallel with writing log entries to the local log.</li><li>Fully concurrent replication: The leader sends log entries to all followers independently and concurrently.</li><li>Asynchronous: The entire workflow of JRaft is free of congestions and is completely asynchronous. It is a typical callback programming model.</li><li>ReadIndex: Raft handles read requests by converting the requests into log entries, and follows the log handling process as described above, which increases the costs and reduces the performance. To solve this problem, in JRaft, the leader records only the commitIndex for each read request, and then sends heartbeats to all peers (followers) to indicate the leader&rsquo;s identity. After the leader&rsquo;s identity is verified, if appliedIndex &gt;= commitIndex, the leader can return the read result to the client. ReadIndex also allows followers to easily implement linearizable reads, but the commitIndex must be obtained from the leader, which requires one more round of RPCs. Linearizable reads will be discussed later in this document.</li><li>Lease read: JRaft also supports maintenance of leader authority through a lease, during which the leader does not have to send heartbeats consecutively to the followers. This reduces the communication overhead and improves the performance. However, due to the timer drift problem, using a timer to maintain a lease is not always safe. Therefore, ReadIndex is used by JRaft by default, because it is safe and its performance is good enough in most cases.</li></ul><h2 id=jraft-architecture>JRaft architecture</h2><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*b3tDQoaFCNkAAAAAAAAAAABjARQnAQ alt="jraft-design | left | 700x550"></p><ul><li>Node: A node in a Raft cluster connects to and encapsulates all underlayer service modules, and main service interfaces that are visible to users. Specifically, the leader node of a raft group calls <code>apply(task)</code> to commit new tasks to the state machine replication cluster made up by the Raft group, which will then apply the task to the business state machine.</li><li>Storage: Modules at the lower part of the preceding figure are storage-related modules.<ul><li>Log storage stores log entries converted from requests that are submitted by Raft users, and replicates log entries from the leader&rsquo;s log to followers&rsquo; logs.<ul><li>LogStorage implements the storage feature and is based on RocksDB by default. You can easily scale up your log storage.</li><li>LogManager is responsible for calling the underlayer storage, caching and batch submitting storage calls, and conducting necessary checks and optimization.</li></ul></li><li>MetaStorage stores the metadata and records the internal states of the Raft implementation, for example, the current term of the node and the node to vote for.</li><li>Snapshot storage (optional) is used to store users&rsquo; state-machine snapshots and meta information.<ul><li>SnapshotStorage stores snapshots.</li><li>SnapshotExecutor manages the actual storage, remote installation, and replication of snapshots.</li></ul></li></ul></li><li>State machine<ul><li>StateMachine is the implementation of users&rsquo; core logic. It calls the <code>onApply(Iterator)</code> method to apply log entries that are submitted with <code>Node#apply(task)</code> to the business state machine.</li><li>FSMCaller encapsulates state transition calls that are sent to the User StateMachine, writes log entries, implements a finite-state machine (FSM), conducts necessary checks, and merges requests for batch submission and concurrent processing.</li></ul></li><li>Replication<ul><li>Replicator is used by the leader to replicate log entries to followers. It does the same thing as an AppendEntries RPC of Raft. Without log entries, it is sent by the leader as heartbeats.</li><li>ReplicatorGroup is used by a Raft group to manage all replicators, and to perform necessary permission checks and dispatches.</li></ul></li><li>RPC: The remote procedure call (RPC) module is used for network communication between nodes.<ul><li>RPC Server: The RPC server is built in a node to receive requests from other nodes or clients, and to redirect such requests to the corresponding service modules.</li><li>RPC Client: The RPC Client is used to issue requests to other nodes, such as requests for votes, log replication requests, and heartbeats.</li></ul></li><li>KV Store: The KV Store is a typical application scenario of various Raft implementations. JRaft contains an embedded distributed KV storage service (JRaft-RheaKV).</li></ul><h3 id=jraft-group>JRaft Group</h3><p>A single JRaft node is meaningless. See the architecture diagram of a three-replica JRaft cluster as follows.</p><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*1wYDQJvcbSEAAAAAAAAAAABjARQnAQ alt="jraft-group | left | 700x550"></p><h3 id=jraft-multi-group>JRaft Multi Group</h3><p>A single raft group cannot solve the read/write bottleneck of large traffic. Therefore, JRaft supports multi-raft-group.</p><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*DFDHRbIAh0sAAAAAAAAAAABjARQnAQ alt="jraft-multi-group | left | 700x550"></p><h2 id=jraft-implementation-details-analysis-highly-efficient-linearizable-reads>JRaft implementation details analysis - highly efficient linearizable reads</h2><p>Introduction to linearizable reads Here is a simple example. Assume that we write a value at the moment t1. We can certainly read this value after t1, but we cannot read values that are written earlier than t1. Think about the Java volatile keyword. Simply put, linearizable reads are an implementation of the Java volatile semantics on a distributed system.</p><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*hVE6RZ9SElEAAAAAAAAAAABjARQnAQ alt="read-only-safe | left | 700x250"></p><p>As shown in the above figure, clients A, B, C, and D all satisfy the requirement of linearizable reads. Client D may seem to be a stale read, but it is not. The request of client D crosses three stages, but the read can occur at any time, so it can either read one or two.</p><p><strong>Important: The following discussion is based on a precondition that the implementation of the business state machine meets the requirement of linearizable reads, or the Java volatile semantics.</strong></p><ul><li><p>To implement linearizable reads, we may want to be straight forward and directly read from the current leader node.</p><ul><li>It does not work, because you do not know whether this &ldquo;leader&rdquo; is still the leader. For example, in the case of network partitioning, it may have lost its leadership position without noticing it.</li></ul></li><li><p>The simplest implementation is to undergo the Raft protocol (Raft log process) like &ldquo;writing&rdquo; requests.</p></li></ul><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*ofC8QJB_2McAAAAAAAAAAABjARQnAQ alt="raft-log | left | 400x160">
* It surely works, if you accept undermined performance. The Raft log process involves log I/O overhead, network overhead of log replication, and disk usage overhead of Raft &ldquo;log reads&rdquo;, which are usually unacceptable in read-intensive systems.</p><ul><li><p>ReadIndex Read</p><ul><li>This is an optimization that is mentioned in the Raft paper. Specifically,<ul><li>(1) The leader records the current commitIndex of its log in a local variable ReadIndex.</li><li>(2) The leader sends a round of heartbeats to followers. If more than a half of the nodes respond to the heartbeats, the leader is sure that it is still the leader.</li><li>(3) The leader keeps waiting until the applyIndex is greater than the ReadIndex, when it can safely provide linearizable reads without worrying about its leadership position. (Think why the leader has to wait for its applyIndex to become greater than the ReadIndex.)</li><li>(4) The leader executes the read request, and returns the result to the client.</li></ul></li><li>ReadIndex also allows followers to easily implement linearizable reads:<ul><li>A follower requests for the latest ReadIndex from the leader.</li><li>The leader executes the first three steps to make sure it is still the leader and returns the ReadIndex to the follower.</li><li>The follower waits until its applyIndex is greater than the ReadIndex.</li><li>The follower executes the read request, and returns the result to the client.
You can configure whether clients can read from the followers in JRaft. This option is disabled by default.</li></ul></li><li>Summary of ReadIndex<ul><li>Compared with the Raft log method, ReadIndex saves disk overhead and can greatly improve the throughput. When you use ReadIndex in combination with JRaft&rsquo;s batch + pipeline ack + full asynchronous mechanisms, in the case of a three-replica cluster, the read throughput of the leader is approximate to the maximum RPC throughput.</li><li>The latency is determined by the slowest heartbeat response of the majority of nodes. Therefore, technically, this method does not significantly reduce the latency.</li></ul></li></ul></li><li><p>Lease read</p><ul><li>Lease read is similar to ReadIndex, but it does more. It not only makes logs unnecessary, but also the network communication. It significantly reduces the latency and in the meantime improves the throughput.</li><li>The basic idea is that the leader takes a lease period that is smaller than election timeout (preferably one order of magnitude smaller). There will be no election during the lease period, which means the leadership does not change in this period. Therefore, the second step of ReadIndex can be skipped to reduce the delay. You can see that the accuracy of lease read is connected with the time. Therefore, the time is vitally important to this mechanism. In the case of a serious timer drift, the mechanism may cause problems.</li><li>Implementation method:<ul><li>The leader sends timed heartbeats and receives responses from a majority of the nodes to ensure the validity of its leadership. In JRaft, the default interval of heartbeats is one tenth of the election timeout.</li><li>During a validity lease period, the current leader is the one and only legitimate leader within the Raft group, and the second step of the ReadIndex mechanism, that is, heartbeat-based identity verification, can be ignored.</li><li>The leader keeps waiting until the applyIndex is greater than the ReadIndex before it safely provides linearizable reads.</li></ul></li></ul></li><li><p>One step further: wait free</p><ul><li>So far, lease read has skipped the second step of ReadIndex. Actually, it can do more by skipping the third step.</li><li>Think about the nature of the preceding implementation:<ul><li>First, we define two states: log state (log_state) and state machine state (st_state). log_state of the leader reflects the latest data state of the current Raft group, because all write requests must first be written to the leader&rsquo;s Raft log.</li><li>When the leader receives a read request, it takes the log_state as a reference logical time point. When st_state catches up with the log_state, we can be sure that all existing data available at the time of the log state has been completely applied to the state machine. Therefore, the linearizability is guaranteed as long as your business state machine remains visible.</li><li><strong>Therefore, the nature of the preceding implementation is to wait for the st_state to catch up with or surpass the state when the leader receives the read request (applyIndex &gt;= commitIndex).</strong></li></ul></li><li>Based on above analysis, we can see that the condition applyIndex &gt;= commitIndex is actually very conservative. <strong>Technically, we only need to ensure that the st_state is the latest at the current time point.</strong></li><li>The problem is whether we can ensure that the st_state of the leader node is always the latest.<ul><li>First of all, the log of the leader state is surely the latest. Even a new leader is elected, the log of the new leader must still contain all committed log entries. However, the state machine of the new leader may lag behind that of the former leader.</li><li>However, when the leader applies the first log entry to its state machine within its current term, its state machine becomes the latest.</li><li>Therefore, we can conclude that, when a leader successfully applies the first log entry to its state machine within its current term, it can immediately execute the read request, which is surely linearizable, without obtaining the commitIndex or waiting for the state machine.</li></ul></li><li>Summary: The wait-free mechanism will minimize the read delay. Although this mechanism has not been implemented in JRaft, it is in our plan.</li></ul></li></ul><p>Example code for initiating a linearizable read request in JRaft:</p><pre><code class=language-text>// Implement linearizable read of KV data.
public void readFromQuorum(String key, AsyncContext asyncContext) {
    // Pass the request ID as the context of the request.
    byte[] reqContext = new byte[4];
    Bits.putInt(reqContext, 0, requestId.incrementAndGet());
    // Call the ReadIndex method, and wait for the callback.
    this.node.readIndex(reqContext, new ReadIndexClosure() {

        @Override
        public void run(Status status, long index, byte[] reqCtx) {
            if (status.isOk()) {
                try {
                    // The ReadIndexClosure callback is successful. The latest data can be read from the state machine and returned to the client.
                    // If your state implementation is subject to version control, you can read the data based on the log index ID.
                    asyncContext.sendResponse(new ValueCommand(fsm.getValue(key)));
                } catch (KeyNotFoundException e) {
                    asyncContext.sendResponse(GetCommandProcessor.createKeyNotFoundResponse());
                }
            } else {
                // In special cases, for example a leader election, the read request fails.
                asyncContext.sendResponse(new BooleanCommand(false, status.getErrorMsg()));
            }
        }
    });
}
</code></pre><h1 id=application-scenarios>Application scenarios</h1><ol><li>Leader election</li><li>Distributed lock services such as ZooKeeper have provided complete distributed lock implementation in the RheaKV module of JRaft</li><li>Highly reliable meta information management based on JRaft-RheaKV</li><li>Distributed storage system, such as distributed message queue, distributed file system, and distributed block system</li></ol><h2 id=use-cases>Use cases</h2><ol><li>RheaKV: an embedded, distributed, highly available, and strongly consistent KV storage class library implemented based on JRaft.</li><li>AntQ Streams QCoordinator: a coordinator using JRaft to implement elections in the Coordinator cluster and using JRaft-RheaKV for meta information storage.</li><li>Schema Registry: a very reliable schema management service (similar to Kafka Schema Registry) with its storage module implemented based on JRaft-RheaKV.</li><li>Metadata management module of SOFARegistry: a IP address registration system. The data held by all nodes must be consistent, and the normal data storage must not be affected when a minority of nodes fail.</li></ol><h2 id=practices>Practices</h2><h3 id=1-design-a-simple-kv-store-based-on-jraft>1. Design a simple KV Store based on JRaft</h3><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*D1N5TZSqQlgAAAAAAAAAAABjARQnAQ alt="kv | left | 700x550"></p><h3 id=2-jraft-based-architecture-of-rheakv>2. JRaft-based architecture of RheaKV</h3><p><img src=https://gw.alipayobjects.com/mdn/rms_da499f/afts/img/A*6K1mTq0z-TkAAAAAAAAAAABjARQnAQ alt="rheakv | left | 700x550"></p><h4 id=terms-and-definitions>Terms and definitions</h4><p><strong>PD</strong>: The global central master node that is responsible for scheduling the entire cluster. You do not need to enable PD for clusters that do not require automatic management. A PD can manage multiple clusters, with each of them isolated by clusterid.</p><p><strong>Store</strong>: A physical storage node within a cluster. A store may contain one or more regions.</p><p><strong>Region</strong>: The minimal KV data unit. Each region has a left closed and right open interval [startKey, endKey), which supports automatic split and automatic backup migration by metrics such as request traffic, load, and data volume.</p><h4 id=characteristics>Characteristics</h4><ul><li>Embedded</li><li>Strong consistency</li><li>Self-driven<ul><li>Automatic diagnosis, optimization, and decision making</li></ul></li></ul><p>The above characteristics, especially the second and the third, are basically implemented based on JRaft&rsquo;s own features. For more information, see JRaft documentation.</p><h1 id=jraft-documentation-https-github-com-alipay-sofa-jraft-wiki><a href=https://github.com/alipay/sofa-jraft/wiki>JRaft documentation</a></h1><h1 id=acknowledgement>Acknowledgement</h1><p>Many thanks to outstanding Raft implementations, including <a href=https://github.com/brpc/braft>Braft</a>, <a href=https://github.com/etcd-io/etcd>Etcd</a>, and <a href=https://github.com/tikv/tikv>TiKV</a>, which have benefited JRaft a lot.</p><h1 id=recruitment>Recruitment</h1><p>Ant Financial Middleware has been looking for talents who have a passion for basic middleware (such as message service, data middleware, and distributed computing) and the next generation high-performance time-series database for real-time analysis. If you are interested, feel free to contact boyan@antfin.com.</p><h1 id=references>References</h1><ul><li><a href=https://github.com/alipay/sofa-jraft>JRaft source code</a></li><li><a href=https://raft.github.io/>https://raft.github.io/</a></li><li><a href=https://raft.github.io/slides/raftuserstudy2013.pdf>https://raft.github.io/slides/raftuserstudy2013.pdf</a></li><li><a href=https://github.com/hedengcheng/tech/tree/master/distributed>Paxos/Raft: Theoretical analysis and practical use of distributed consensus algorithms</a></li><li><a href=https://github.com/brpc/braft/blob/master/docs/cn/raft_protocol.md>Braft documentation</a></li><li><a href=https://pingcap.com/blog-cn/linearizability-and-raft/>https://pingcap.com/blog-cn/linearizability-and-raft/</a></li><li><a href=https://aphyr.com/posts/313-strong-consistency-models>https://aphyr.com/posts/313-strong-consistency-models</a></li><li><a href=https://zhuanlan.zhihu.com/p/51063866>https://zhuanlan.zhihu.com/p/51063866</a></li></ul></article></main></div><footer class=ss-footer><div class=container><div class=links><div class=cate><h2 class=cate-title>Resources</h2><a class=link href=https://github.com/dromara>Github</a>
<a class=link href=https://gitee.com/shuaiqiyu>Gitee</a></div><div class=cate><h2 class=cate-title>Get Involved</h2><a class=link href=https://github.com/dromara/soul/issues/new>Feedback</a>
<a class=link href=/community>Community</a>
<a class=link href=/blog>Blog</a></div><div class=cate><h2 class=cate-title>Document</h2><a class=link href=/projects/soul/overview/>Soul</a>
<a class=link href=/projects/hmily/overview/>Hmily</a>
<a class=link href=/projects/raincat/overview/>Raincat</a>
<a class=link href=/projects/myth/overview/>Myth</a></div></div><div class=qrcode><div><img class=qrcode-img src=/img/qrcode/qrcode_1.png><p class=qrcode-desc>Wechat Official Account</p></div><div><img class=qrcode-img src=/img/qrcode/qrcode_2.png><p class=qrcode-desc>QQ Group</p></div></div></div><div class=copyright><p>Copyright ©2021
<a href=/>xiaoyu@apache.org by xiaoyu</a></p></div></footer></body></html>